<!doctype html>
<html lang="en">
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<!-- Prevent, to some degree, the execution of inline JavaScript, as well as blocking all plugin content  -->
		<meta http-equiv="Content-Security-Policy" content="script-src 'none'; object-src 'none'; img-src 'none'; font-src 'none'; media-src 'none'; worker-src 'none'; connect-src 'none'; style-src 'self' ">
		<title>A Robust Open Ecosystem for All: Accelerating AI Infrastructure</title>
		<link rel="icon" href="favicon.png" />
		<link rel="stylesheet" href="/a11y-dark.css">
		<link rel="stylesheet" href="/style.css">
	</head>
	<body>
		<main>
		<a target="_blank" rel="noreferrer" href="http://opensource.googleblog.com/2024/12/a-robust-open-ecosystem-accelerating-ai-infrastructure.html">Go to article URL</a>
		<div id="content"><p></p><meta content="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8ULqW7hZEMBHD9yWCR9KXIivY5HI1PKOSabXvei2Xjc-4DvbIchNxDuQjRXzX-RozrEUPekmRndDDoaY-I3TV0FZ9tc4WZj_0-CJUcfUtz3hFhyMOZyzpHrGGIz1ewexG8aIL557xBHkl4YyOB0osjnZtg9OiRwEWh93FvInpALrpliJyGBbnIkvGbME/s1600/Robust-Open-Democratizing-social-multi@2x.png" name="twitter:image">


<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhalbL5y4FkZ0cjRqnaRLsCiTYVkJMEAKWaqj87DiGQIJzJAJUGibXnBJkXHbJmZ9msttJ8_wHwzzkO8sQQloqzQZvjHfiGqDbwGWXSd5ZnE5-0iQJZynuTytoxBPlo0U-rDBlkUC5k8UtpAHZwjo2kysVQJaQrZhr_vrkMTOelO8YJwkVuh8O9xISkXLY/s1600/Robust-Open-Democratizing-editorial-multi@2x.png" rel="noreferrer" target="_blank"></a>

<div><br></div>
  
<h2><b>JAX now runs on AWS Trainium: Open Source Fuels AI Innovation</b></h2>

<p>Open source software is the foundation of machine learning. It accelerates innovation through an ethos of flexibility and collaboration. This philosophy drives the open development of <a href="https://jax.readthedocs.io/en/latest/" target="_blank" rel="noreferrer">JAX</a>, our high-performance array computing library, as well as <a href="https://openxla.org" target="_blank" rel="noreferrer">OpenXLA</a>, the compiler and runtime infrastructure it relies on.</p>

<p>Today we're excited to highlight how this commitment to openness, together with JAX and <a href="https://openxla.org/" target="_blank" rel="noreferrer">OpenXLA</a>'s modular designs, enables seamless integration of AWS Trainium and Trainium2 chips accelerators into the JAX ecosystem. Users get more portability, more choice, and faster progress.</p><br>

<h2>JAX and OpenXLA, abstraction and modularity</h2>

<p><a href="https://jax.readthedocs.io/en/latest/" target="_blank" rel="noreferrer">JAX</a> is a Python library for high-performance, large-scale numerical computing and machine learning. Its unique compiler-oriented design makes numerical computation familiar and portable while also accelerator-friendly and scalable. It combines a NumPy-like API with composable transformations for automatic differentiation, vectorization, parallelization, and more. Under the hood, JAX leverages the XLA compiler to optimize and scale computations over a broad set of backends.</p>

<p>This abstraction layer is key to its portability: JAX presents a consistent interface while XLA optimizes performance, whether you're running on CPUs, GPUs, TPUs, or something new.</p>

<p>In fact, <a href="https://openxla.org/" target="_blank" rel="noreferrer">OpenXLA</a> infrastructure is <a href="https://opensource.googleblog.com/2024/03/pjrt-plugin-to-accelerate-machine-learning.html" target="_blank" rel="noreferrer">designed to be modular</a> and extensible to new platforms. By developing a <a href="https://openxla.org/xla/pjrt/pjrt_integration" target="_blank" rel="noreferrer">PJRT plugin</a> and leveraging existing XLA compiler components, JAX code can target new platforms, even when scaling from a single device to thousands.</p><br>

<h2>Enter AWS Trainium and Inferentia</h2>

<p>We are excited to announce that <a href="https://press.aboutamazon.com/2024/12/aws-trainium2-instances-now-generally-available" target="_blank" rel="noreferrer">AWS Trainium is the latest platform to embrace JAX and OpenXLA</a>. With <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/jax/setup/jax-setup.html" target="_blank" rel="noreferrer">the JAX Neuron plugin</a>, AWS Trainium and Inferentia can be used as native JAX devices.</p>

<p>This new backend demonstrates how abstraction and modularity make JAX and OpenXLA especially extensible and amenable to collaboration, even on new hardware. We're thrilled to have diverse hardware partners like AMD, Arm, Intel, Nvidia, and AWS taking advantage of JAX's portability and performance. If you're interested in bringing new platforms into the JAX and OpenXLA ecosystem, <a href="https://groups.google.com/a/openxla.org/g/openxla-discuss" target="_blank" rel="noreferrer">please reach out</a>!</p>

<p>A multi-platform ecosystem fosters open collaboration in advancing AI infrastructure. Our goal is to drive continuous development of open standards and to accelerate progress. And if you're a machine learning developer or numerical computing user, we're excited for you to try <a href="https://jax.readthedocs.io/en/latest/" target="_blank" rel="noreferrer">JAX</a> on any platform you choose.</p>

<p><i>By Matthew Johnson - Principal Scientist, with additional contributors: Aditi Joshi, Fenghui Zhang, Roy Frostig, and Carlos Araya</i></p>

<p></p></div>
		<div class="content-meta">
			<time datetime=2024-12-03T09:00:00.000Z>3 December 2024</time>
			<a href="/urls/feeds-feedburner-com-googleopensourceblog">feeds.feedburner.com/GoogleOpenSourceBlog</a>
			<div> <a href="/tags/open-source.html">open-source</a> |  <a style="font-weight: bold" href="/tags/source.html">source</a></div>
		</div>
	</main>
		<footer>
			<nav>
				<a href="/">Home</a>
				<a href="/tags">Tags</a>
				<a href="/urls">URLs</a>
				<a rel="noreferrer" target="_blank" href="https://github.com/thoughtsunificator/rss-feed-static-generator">Source code</a>
			</nav>
		</footer>
		<script src="/highlight.min.js"></script>
		<script>hljs.highlightAll();</script>
		</body>
</html>